\chapter*{Key Terms}
\addcontentsline{toc}{chapter}{Key Terms}
%give me a solution for this tex file to remove the space shown between the text at \textbf and the ':' 

\begin{itemize}
 \item Attention: Statistical apparatus for evaluating the impact of each token fed through an LLM.
\item \textbf{Cohere:} is a Canadian multinational technology company focused on enterprise artificial intelligence, specializing in large language models.
\item \textbf{Deep Learning}: The field within ML that is focused on unstructured data, which includes text and images. It relies on artificial neural networks, a method that is (loosely) inspired by the human brain. Deep learning has revolutionized various domains, including computer vision, natural language processing, and speech recognition, by achieving state-of-the-art performance in many tasks.

\item \textbf{Embeddings:} Numerical representations of words that capture their meanings and relationships in terms of context.

\item \textbf{GPT (Generative Pre-trained Transformer):} A type of large language model that utilizes transformer architecture and is pre-trained on vast amounts of text data. GPT models are capable of generating coherent and contextually relevant text, making them useful for a wide range of natural language processing tasks.

\item \textbf{Gemini:} Large language model under ongoing development, Google trained to be informative and comprehensive. It can access and process information from the real world through Google Search.

\item \textbf{Gemini Pro:} Version within the Gemini family of large language models, created by Google DeepMind.
\item \textbf{HuggingFace}:French-American startup in the field of Artificial Intelligence founded in 2015, developing tools to use machine learning.

\item \textbf{Instruction Tuning:} Training a language model to answer different prompts to learn how to answer new ones. This process involves fine-tuning the model's parameters based on the specific prompts provided during training.

\item \textbf{LLaMa.cpp:} Developed by Georgi Gerganov, LLaMa.cpp implements the Meta's LLaMa architecture in efficient C/C++. It is one of the most dynamic open-source communities around the LLM inference with more than 390 contributors, 43000+ stars on the official GitHub repository, and 930+ releases. LLaMa.cpp is widely used for LLM inference tasks due to its efficiency and active community support.

\item \textbf{Machine Learning (ML):} A subfield of AI that specifically focuses on pattern recognition in data. ML algorithms learn from data to identify patterns and make predictions or decisions without explicitly programming them for specific tasks.
\item \textbf{OpenAI:} is a company specializing in artificial reasoning, with a 'capped for-profit' mission, headquartered in San Francisco. Before March 2019, it was recognized as a nonprofit organization.
\item \textbf{PEFT (Parameter-Efficient Fine-Tuning):} A library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a modelâ€™s parameters because it is prohibitively costly. PEFT helps streamline the fine-tuning process by focusing on specific parameters relevant to the downstream task, thus reducing computational resources and time.

\item \textbf{Prompt:} The input a user provides to an LLM to elicit a response or carry out a task. It serves as a guiding instruction for the model to generate the desired output.

\item \textbf{RLHF (Reinforcement Learning with Human Feedback):} A technique that tunes a model based on human preferences. RLHF leverages reinforcement learning algorithms to adjust the model's parameters in response to human feedback, aiming to improve its performance.

\item \textbf{RAG:} Technique that complements text generation with information from private or proprietary data sources.

\item \textbf{Transformers}: A neural network architecture that forms the basis of most Large Language Models (LLMs). Transformers are particularly effective in handling sequential data and have been instrumental in advancing various natural language processing tasks.


    \item \textbf{MMLU (Massive Multitask Language Understanding):} A new benchmark designed to measure knowledge acquired during pre-training by evaluating models exclusively in zero-shot and few-shot settings.
    
    \item \textbf{ARC (A12 Reasoning Challenge):} Conceived by Clark et al. in 2018 as a rigorous test for Large Language Models' (LLMs) question-answering capabilities.
    
    \item \textbf{HellaSwag benchmark:}Introduced by Zellers et al., stands for "Harder Endings, Longer Contexts, and Low-shot Activities for Situations with Adversarial Generations." It serves as a challenging evaluation for commonsense reasoning abilities in large language models (LLMs).
    
    \item \textbf{TruthfulQA:}TruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance, and politics. The authors crafted questions that some humans would answer falsely because of a false belief or misconception.
    
    \item \textbf{Winograd Schema Challenge (WSC):} Introduced by Levesque, Davis, and Morgenstern in 2011, is indeed a benchmark designed for evaluating commonsense reasoning abilities in natural language understanding systems. It consists of a set of carefully crafted pronoun resolution problems, which were specifically created to be challenging for statistical models relying solely on selectional preferences or word associations.
    
    \item \textbf{GSM8K:} A dataset of 8.5K high quality linguistically diverse grade school math word problems created by human problem writers. The dataset is segmented into 7.5K training problems and 1K test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations ($+$, $\textminus$, $\times$, $\div$) to reach the final answer. A bright middle school student should be able to solve every problem. It can be used for multistep mathematical reasoning.
    
    \item \textbf{Artificial general intelligence (AGI):} A theoretical field of AI research that attempts to create software that has human-like intelligence and is capable of self-learning.
\end{itemize}
